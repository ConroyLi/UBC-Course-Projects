\documentclass[a4paper,12pt]{article} % The document class with options
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{microtype}

\begin{document}

\section{\textbf{Different categories of PDEs}}

Partial Differential Equations (PDEs) are a class of equations that involve the partial derivatives of a function of multiple variables. They are fundamental in expressing a variety of physical, engineering, and mathematical phenomena. There are several different kinds of PDEs, each with unique characteristics and applications. Here are some of the most common types:

\begin{enumerate}
\item Elliptic Equations:
\begin{itemize}
   \item Example: Laplace's Equation, \(\nabla^2 u = 0\), and Poisson's Equation, \(\nabla^2 u = f\).
   \item Characteristics: No time dependence, solutions are general 
   smooth and describe equilibrium states.
   \item Applications: Steady-state heat distribution, electrostatics, incompressible fluid flow.
\end{itemize}
\item Parabolic Equations:
\begin{itemize}
   \item Example: Heat Equation, \(\frac{\partial u}{\partial t} = \nabla^2 u\).
   \item Characteristics: Contains time derivative and spatial derivatives; models phenomena that evolve over time towards an equilibrium.
   \item Applications: Heat conduction, diffusion processes.
\end{itemize}
\item Hyperbolic Equations:
\begin{itemize}
   \item Example: Wave Equation, \(\frac{\partial^2 u}{\partial t^2} = c^2 \nabla^2 u\).
   \item Characteristics: Second-order in time, describe wave propagation and vibrations.
   \item Applications: Acoustics, electromagnetic waves, seismic waves.
\end{itemize}

\item Transport (or Convection-Diffusion) Equations:
\begin{itemize}
   \item Example: \(\frac{\partial u}{\partial t} + v \cdot \nabla u = D \nabla^2 u\), where \(v\) is velocity and \(D\) is the diffusion coefficient.
   \item Characteristics: Describe phenomena involving both transport (or advection) and diffusion.
   \item Applications: Fluid dynamics, pollutant dispersion.
\end{itemize}

\item Nonlinear Differential Equations:
\begin{itemize}
   \item Example: Nonlinear Schr√∂dinger Equation, Korteweg-de Vries Equation.
   \item Characteristics: The equation includes nonlinear terms (products or powers of the function and its derivatives).
   \item Applications: Complex physical phenomena, including solitons, fluid dynamics, and optical physics.
\end{itemize}

\item Mixed Type Equations:
\begin{itemize}
   \item Example: Tricomi Equation.
   \item Characteristics: The equation changes type (from elliptic to hyperbolic, for instance) within the domain.
   \item Applications: Transonic flow, certain problems in gas dynamics.
\end{itemize}

\item Eigenvalue Problems:
\begin{itemize}
   \item Example: \(-\nabla^2 u = \lambda u\) (Helmholtz equation in eigenvalue form).
   \item Characteristics: Involves finding a function \(u\) and a number \(\lambda\) (eigenvalue) such that the equation is satisfied.
   \item Applications: Quantum mechanics, stability analysis, structural engineering.
\end{itemize}

\end{enumerate}
Each of these types of PDEs plays a crucial role in modeling different physical phenomena. The solution techniques and analytical approaches vary significantly among these types, reflecting the diverse nature of the phenomena they model. Understanding the specific type of PDE is essential in choosing the right methods for analysis and numerical simulation.

\subsection{\textbf{Definition}}
\begin{enumerate}
   \item The equation is called elliptic at the point x provided
\(A(x)\) is positive definite.
   \item The equation is called hyperbolic at the point x provided \(A(x)\) has one negative and \(n - 1\) positive eigenvalues.
   \item The equation is called parabolic at the point x provided \(A(x)\) is positive semidefinite, but is not positive definite, and the rank of \((A(x), b(x))\) equals n.
   \item An equation is called elliptic, hyperbolic or parabolic provided it has the corresponding property for all points of the domain.
\end{enumerate}


\section{\textbf{I.B.P in higher dimensions}}
Integration by parts in higher dimensions is a generalization of the integration by parts formula from one-dimensional calculus to multiple dimensions. This generalization is often used in vector calculus and is particularly useful in the context of functions over multi-dimensional domains. The concept is closely related to the Divergence Theorem and Green's Theorem, which are fundamental in vector calculus.

In one dimension, the integration by parts formula is given by:

\[ \int u \, dv = uv - \int v \, du \]

For higher dimensions, consider a domain \( D \) in \( \mathbb{R}^n \) and functions \( u \) and \( v \) that are sufficiently smooth (i.e., have continuous derivatives). The integration by parts formula in higher dimensions, specifically for \( n = 2 \) or \( n = 3 \), can be expressed using the gradient, divergence, or curl, depending on the context. Here's a general idea for a domain in \( \mathbb{R}^3 \):

\[ \int_D u \, \nabla v \, dV = \int_{\partial D} u v \, dS - \int_D v \, \nabla u \, dV \]

In this formula:

- \( \nabla v \) is the gradient of \( v \).
- \( dV \) is the volume element in \( \mathbb{R}^3 \).
- \( \partial D \) represents the boundary of the domain \( D \).
- \( dS \) is the surface element on \( \partial D \).

The physical interpretation of this formula involves the flow of a vector field across the boundary of a region and within the region itself.

The specific form of the integration by parts formula will depend on the nature of the functions \( u \) and \( v \) (scalar or vector fields) and the dimension of the space. In vector calculus, various forms of this theorem, like the Divergence Theorem (for divergence and flux) and Green's Theorem (in the plane), are essentially higher-dimensional versions of integration by parts.

It's important to note that applying integration by parts in higher dimensions requires a good understanding of vector calculus, including gradients, divergences, curls, and theorems related to flux and circulation.

\newpage
\section{\textbf{Support of funtion}}

The support of a function, particularly in the context of mathematics and analysis, is a fundamental concept. It refers to the set of points where the function is not zero, or, more precisely, the closure of that set. Let's break down this definition:

\begin{itemize}
   \item Support of a Real-Valued Function: For a real-valued function \( f: \mathbb{R} \rightarrow \mathbb{R} \), the support is the set of points in \( \mathbb{R} \) where \( f(x) \) is not zero. This can be expressed as:

   \[ \text{supp}(f) = \{ x \in \mathbb{R} \,|\, f(x) \neq 0 \} \]

   \item Closure of the Set: More formally, the support of \( f \) is the closure of this set. The closure includes all the points where \( f \) is non-zero, plus any limit points of this set. A limit point is a point where the function approaches but does not necessarily reach zero. This means that the support of a function can include points where the function itself is zero, as long as these points are limit points of the set where the function is non-zero.

   \item Support in Higher Dimensions: The concept of support extends to functions in higher dimensions as well. For a function \( f: \mathbb{R}^n \rightarrow \mathbb{R} \), the support is the closure of the set of points in \( \mathbb{R}^n \) where \( f \) is not zero.

   \item Importance in Various Contexts: The notion of support is important in many areas of mathematics, including analysis, differential equations, and functional analysis. In the context of distributions or generalized functions, the support of a distribution is the set outside of which the distribution acts like the zero distribution.

   \item Compact Support: A function is said to have compact support if its support is a compact set. In \( \mathbb{R}^n \), a set is compact if it is closed and bounded. Functions with compact support are especially important in various applications because they vanish outside of a bounded region, making them easier to handle in many analytical and numerical computations.

\end{itemize}
Understanding the support of a function is crucial when working with concepts such as convolution, Fourier transforms, and in the study of partial differential equations, where the behavior of functions over their domain plays a key role.

\newpage
\section{\textbf{Measure of a set or a function}}
The concepts of the measure of a set and the measure of a function are fundamental in mathematical analysis, particularly in the areas of measure theory and integration. Let's define these two concepts in detail:

\subsection{Measure of a Set}

The measure of a set is a way to assign a non-negative number to certain subsets of a space (like the real line or $\mathbb{R}^n$) in a way that generalizes the concept of length, area, and volume. This is formally defined in the context of measure theory.

Let $(X, \mathcal{M}, \mu)$ be a measure space where $X$ is a set, $\mathcal{M}$ is a $\sigma$-algebra of subsets of $X$ (these are the measurable sets), and $\mu$ is a measure. Then, the measure of a set $A \in \mathcal{M}$, denoted $\mu(A)$, is a non-negative extended real number assigned to $A$ by $\mu$.

The most common example is the Lebesgue measure on $\mathbb{R}$, which generalizes the concept of length. For instance, if $A$ is an interval $[a, b]$, then its Lebesgue measure is $b - a$, the length of the interval.

\subsection{Measure of a Function} 

The measure of a function usually refers to an integral of the function with respect to a certain measure, often used to generalize the concept of the sum, total mass, or total charge of a function over a set.

If $f: X \to \mathbb{R}$ is a measurable function on a measure space $(X, \mathcal{M}, \mu)$, then the measure (or integral) of the function $f$ with respect to the measure $\mu$ over a set $A \in \mathcal{M}$ is denoted as:

\[
\int_A f \, d\mu
\]

This integral can be interpreted as a weighted sum of the values of $f$, with the weights given by the measure $\mu$.

In the special case where $\mu$ is the Lebesgue measure on $\mathbb{R}$ and $f$ is a real-valued function, $\int_A f \, d\mu$ is the Lebesgue integral of $f$ over $A$, which generalizes the concept of the area under the curve.

In summary, the measure of a set is a number that represents the size of the set in a generalized sense, while the measure (or integral) of a function is a generalized sum of the function's values over a set, weighted by a measure.


\newpage
\section{\textbf{Sobolev Space}}

Sobolev spaces are a fundamental concept in functional analysis and partial differential equations, playing a crucial role in modern analysis, particularly in the study of differential equations and variational problems. They generalize the notion of derivatives and integrals to more abstract settings.

\subsection{Definition of Sobolev Space}

A Sobolev space is a vector space of functions equipped with a norm that is a combination of $L^p$-norms of the function itself and its derivatives up to a certain order. Formally, the Sobolev space $W^{k,p}(\Omega)$ is defined as follows:

Consider a domain $\Omega \subseteq \mathbb{R}^n$. The Sobolev space $W^{k,p}(\Omega)$ consists of all functions $u: \Omega \rightarrow \mathbb{R}$ such that $u$ and all its weak derivatives up to order $k$ are in $L^p(\Omega)$. Here, $k \in \mathbb{N}$ (non-negative integers) represents the order of derivatives considered, and $p \in [1, \infty]$ is a parameter that determines the $L^p$ space, which generalizes the concept of integrability and norm.

\subsection{Weak Derivatives}

A key aspect of Sobolev spaces is the concept of weak derivatives. A weak derivative is a generalization of the classical derivative, applicable to a broader class of functions. It is defined in the sense of distributions, which allows for the inclusion of functions that may not be differentiable in the classical sense.

\subsection{Sobolev Norm}

The norm on $W^{k,p}(\Omega)$, denoted $\|\cdot\|_{W^{k,p}(\Omega)}$, is defined by:

\[
\|u\|_{W^{k,p}(\Omega)} = {\left( \sum_{|\alpha| \leq k} \int_{\Omega} |D^{\alpha} u|^p \, dx \right)}^{1/p}
\]

for $1 \leq p < \infty$. Here, $\alpha$ is a multi-index used to denote derivatives, and $D^{\alpha} u$ represents the weak derivative of $u$.

For $p = \infty$, the norm is defined using the essential supremum:

\[
\|u\|_{W^{k,\infty}(\Omega)} = \max_{|\alpha| \leq k} \text{ess sup}_{x \in \Omega} |D^{\alpha} u(x)|
\]

\subsection{Importance and Applications}
\begin{itemize}

   \item Generalizing Classical Spaces**: Sobolev spaces generalize classical spaces like continuous, differentiable, and integrable function spaces. They are particularly important in settings where functions may not be smooth.
  
   \item Partial Differential Equations**: Sobolev spaces are crucial in the study of PDEs, especially in formulating and solving weak solutions of PDEs.
  
   \item Variational Problems**: They are used in the calculus of variations and in the formulation of variational problems, which seek to find functions minimizing certain integrals.

   \item Embedding Theorems**: Sobolev embedding theorems, which relate different Sobolev spaces and establish conditions under which functions in a Sobolev space are also in more regular function spaces, are fundamental in analysis.

\end{itemize}
   Sobolev spaces thus provide a powerful framework for dealing with functions that have derivatives in some generalized sense, allowing for the rigorous study of a broad range of problems in mathematical analysis.

\newpage
\section{\textbf{Compact Subset}}
A compact subset in mathematics, particularly in the context of topology, is a set that has specific properties related to size and closure. The concept of compactness is crucial because it generalizes the notion of a set being closed and bounded (as in the Heine-Borel theorem for \(\mathbb{R}^n\)) to more abstract spaces. Let's define compactness more formally:

\subsection{Definition of Compact Subset}

A subset \( K \) of a topological space \( X \) is said to be \textbf{compact} if it satisfies the following condition:

- Every open cover of \( K \) has a finite subcover.

This means that if you have a collection of open sets whose union contains \( K \), you can always find a finite number of these open sets whose union still contains \( K \). 

\subsection{Intuition and Examples}
\begin{itemize}
   \item Closed and Bounded in \(\mathbb{R}^n\): In the familiar setting of Euclidean spaces (\(\mathbb{R}^n\)), a subset is compact if and only if it is closed (contains all its limit points) and bounded (can be contained within some large ball). This is the content of the Heine-Borel theorem.
   \item General Topological Spaces: In more general spaces, the notion of being "bounded" may not make sense, so compactness is defined purely in terms of open covers.
\end{itemize}

\subsection{Properties of Compact Sets}
\begin{enumerate}
   \item Closed: Compact sets in a Hausdorff space (a type of topological space where distinct points have disjoint neighborhoods) are always closed.
   \item Containment of Subsequences: In metric spaces, compactness is equivalent to every sequence in the set having a convergent subsequence whose limit is in the set. This property is known as sequential compactness.
   \item Continuity and Compactness: A continuous function mapping from a compact space to a Hausdorff space is closed and takes compact sets to compact sets.
   \item Heine-Borel Theorem: In \(\mathbb{R}^n\), a set is compact if and only if it is closed and bounded. This result, however, does not generalize to all topological spaces.
\end{enumerate}

\subsection{Compact Support}
Compact Support: The subscript 0 indicates that the functions have compact support. The support of a function is the closure of the set where the function is non-zero. Saying that a function has compact support means that there is a bounded region outside of which the function is zero. In simple terms, the function is nonzero only in a finite region and vanishes everywhere else.


\section{\textbf{Dual Hilbert Space}}
The dual of a Hilbert space, often called the dual Hilbert space, is a concept from functional analysis, a branch of mathematics. In this context, the dual space of a given Hilbert space is the space of all continuous linear functionals defined on that Hilbert space. Let's break down what this means:
\begin{enumerate}
   \item Hilbert Space: A Hilbert space is a complete, infinite-dimensional vector space equipped with an inner product. It generalizes the notion of Euclidean space and includes functions as well as finite-dimensional vectors.

   \item Linear Functionals: A linear functional on a Hilbert space \( H \) is a linear map from \( H \) to the field of scalars (which is usually the real numbers \( \mathbb{R} \) or the complex numbers \( \mathbb{C} \)). In other words, it's a function \( f: H \rightarrow \mathbb{R} \) (or \( \mathbb{C} \)) that satisfies linearity, i.e., \( f(a\mathbf{x} + b\mathbf{y}) = af(\mathbf{x}) + bf(\mathbf{y}) \) for all \( \mathbf{x}, \mathbf{y} \in H \) and scalars \( a, b \).

   \item Continuity of Functionals: In the context of Hilbert spaces, we are usually interested in continuous linear functionals. A linear functional is continuous if small changes in the input (elements of the Hilbert space) result in small changes in the output (scalar values).

   \item Dual Space: The dual space \( H^* \) of a Hilbert space \( H \) is the set of all continuous linear functionals on \( H \). This space itself forms a vector space, where the operations of addition and scalar multiplication are defined pointwise.

   \item Representation by the Riesz Representation Theorem: One of the remarkable properties of Hilbert spaces is that each continuous linear functional can be uniquely represented by an element of the Hilbert space itself. This is known as the Riesz Representation Theorem. Specifically, for every continuous linear functional \( f \) on a Hilbert space \( H \), there exists a unique element \( y \in H \) such that \( f(x) = \langle x, y \rangle \) for all \( x \in H \), where \( \langle \cdot, \cdot \rangle \) denotes the inner product in \( H \).

The dual space concept is a powerful tool in functional analysis and is widely used in various areas of mathematics and physics, particularly in the study of partial differential equations and quantum mechanics. The elegance of the dual space of a Hilbert space lies in its isomorphism to the Hilbert space itself, a property not generally true for dual spaces of more general vector spaces.
\end{enumerate}

\newpage
\section{\textbf{why ${H^1}_{0}(a ,b)$ is equvalient to closure ${C^1}_{0}([a ,b]$) }}


The statement that \( H_0^1(a, b) \) is equivalent to the closure of \( C_0^1([a, b]) \) refers to a fundamental concept in functional analysis, particularly in the context of Sobolev spaces. Let's break down what this means:
\begin{enumerate}
   \item Sobolev Space \( H_0^1(a, b) \): This is a space of functions whose first derivatives are square-integrable over the interval \((a, b)\), and which vanish at the endpoints \( a \) and \( b \). More formally, a function \( u \) belongs to \( H_0^1(a, b) \) if \( u \) is in \( L^2(a, b) \), its weak derivative is also in \( L^2(a, b) \), and \( u(a) = u(b) = 0 \).

   \item Space \( C_0^1([a, b]) \): This denotes the space of continuously differentiable functions on the interval \([a, b]\) that are zero at the endpoints \( a \) and \( b \). These are functions that are smooth (continuously differentiable) and vanish at the boundaries of the interval.

   \item Closure of \( C_0^1([a, b]) \): The closure of \( C_0^1([a, b]) \) in the \( H^1 \) norm refers to the smallest closed set in the \( H^1 \) norm that contains all functions in \( C_0^1([a, b]) \). Essentially, this means we include not only the functions in \( C_0^1([a, b]) \) but also the limits of sequences of such functions (in the \( H^1 \) norm).

   \item Equivalence: The equivalence of \( H_0^1(a, b) \) and the closure of \( C_0^1([a, b]) \) is based on the fact that any function in \( H_0^1(a, b) \) can be approximated arbitrarily closely (in the \( H^1 \) norm) by functions in \( C_0^1([a, b]) \). In other words, the closure of \( C_0^1([a, b]) \) in the \( H^1 \) norm includes all functions in \( H_0^1(a, b) \). This is a result of the properties of Sobolev spaces and the concept of weak derivatives.

The equivalence is significant in the study of partial differential equations and functional analysis because it allows the use of smoother functions (from \( C_0^1 \)) to approximate and study the properties of functions in the Sobolev space \( H_0^1 \), which may not be classically differentiable everywhere. This approximation is fundamental in the weak formulation of differential equations and variational methods.

So, in the page of 12 of my note, u in (Pw) belongs to $C^1_0$ originally, but $(u_n)_n$ has not limit(u itself) in $C_0^1$, so ${C^1}_0$ is not complete. Thus, we need to find u in ${H^1}_0$, so that $(u_n)_n$ has limit of $u$.
\end{enumerate}

\newpage
\section{\textbf{Singularuty}}
In mathematics, a singularity in a function refers to a point at which the function does not behave in a regular or predictable manner. Singularities are points where a function may not be defined, may not be differentiable, or may exhibit some form of "infinite" behavior. There are several types of singularities, commonly categorized as follows:
\begin{enumerate}
   \item Removable Singularities**: A point where the function is not defined, but if it were redefined at that point, it could become continuous. For example, the function \( f(x) = \frac{\sin(x)}{x} \) has a removable singularity at \( x = 0 \), since \( \lim_{x \to 0} f(x) = 1 \). By defining \( f(0) = 1 \), the function becomes continuous.

   \item Pole or Pole Singularity**: This is a point where the function goes to infinity. For example, in the function \( f(x) = \frac{1}{x} \), there is a pole at \( x = 0 \) since \( f(x) \) approaches infinity as \( x \) approaches 0. The order of the pole is determined by how rapidly the function approaches infinity.

   \item Essential Singularity**: At an essential singularity, the function behaves in an extremely erratic way. The classic example is given by the function \( f(z) = e^{\frac{1}{z}} \) at \( z = 0 \). Near an essential singularity, a function can take on almost any value with no predictable pattern.

   \item Branch Point**: A point where a function is multi-valued. For example, the function \( f(z) = \sqrt{z} \) has a branch point at \( z = 0 \), since square roots have two values (positive and negative).

   \item Algebraic Singularity**: This occurs in algebraic functions at points where the denominator is zero or where both the numerator and denominator are zero, but the limit is not defined or is infinite.

   \item Isolated and Non-Isolated Singularities**: An isolated singularity is one where no other singularities exist in its immediate vicinity, whereas non-isolated singularities have other singularities arbitrarily close to them.

Understanding and classifying singularities is important in complex analysis, differential equations, and mathematical physics, as they often reveal important properties of functions and the systems they describe. For example, in physics, singularities can correspond to points where a physical theory breaks down (such as the singularity at the center of a black hole in general relativity).
\end{enumerate}
\end{document}